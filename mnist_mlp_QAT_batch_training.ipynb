{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    This script trains batches of multi-layer perceptrons with user-specified structure and digitization options. In order to find the model most resilient to photon shot noise, the training hyperparameters were randomly sampled and a mixture of training strategies (e.g., data augmentation, random digitization etc.) were employed. The best model was selected by testing models against simulated photon shot noise, which is given an example of in ./model_evaluation_shot_noise_sim.ipynb. The script requires package ray and multiple GPUs for parallel training of multiple models, optuna package for parameter searching, and wandb package to log the training results. This is the original script that resulted in the model used in the experiment, which is ./RA_4bit_H2_100_100_lr_0.043_0.50_m_0.87_wep_6_randActDigi_v80_ep97.pt\n",
    "    \n",
    "    For the minimalist training script using only Pytorch (without hyperparameter searching or results logging), please see ./main_mnist_mlp_QAT.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os, sys\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "\n",
    "import wandb # logging training files, optional. If not available, set args.wandb = False\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-17 12:33:40,059\tINFO services.py:1166 -- View the Ray dashboard at http://127.0.0.1:8265\n"
     ]
    }
   ],
   "source": [
    "# Load functions for multiple GPU parallelization\n",
    "exec(open(\"../ana_lib/gpu_par.py\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Training Structure\n",
    "\n",
    "There are three nested loops in a neural architecture search project:\n",
    "\n",
    "    loop around sets of hyperparamters:\n",
    "        loop around epoches for training of a model of a particular set of hyperparameters:\n",
    "            loop around mini-batches in an epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Training and hyperparameter search configurations \"\"\"\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                    help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                    help='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--epochs', type=int, default=100, metavar='N',\n",
    "                    help='number of epochs to train (default: 100)')\n",
    "parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                    help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "                    help='SGD momentum (default: 0.5)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--gpus', default=0,\n",
    "                    help='gpus used for training - e.g 0,1,3')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--wandb', action='store_true', default=True, \n",
    "                    help='enables wandb logger')  \n",
    "parser.add_argument('--csv', action='store_true', default=False, \n",
    "                    help='enables csv logger')  \n",
    "args = parser.parse_args(\"\")\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "# Set random seeds to reproduce results\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Prepare data loaders \"\"\"\n",
    "\n",
    "def conv1d(NCHW_tensor):\n",
    "    conv_ker = torch.tensor([[0.05, 0.1, 0.05], [0.1, 1, 0.1], [0.05, 0.1, 0.05]])\n",
    "    conv_ker = conv_ker.view(1, 1, conv_ker.size(0), conv_ker.size(1))\n",
    "    img_conv1d = F.conv1d(NCHW_tensor.unsqueeze(0), conv_ker, padding=1).squeeze(0)\n",
    "    return img_conv1d/img_conv1d.max()\n",
    "\n",
    "# Data Augmentation with random affine transformation and 2D convolution\n",
    "transforms_distort = transforms.Compose([transforms.RandomAffine(5, translate=(0.04, 0.04), scale=(0.96, 1.04)), \\\n",
    "                                         transforms.ToTensor(), \\\n",
    "                                         transforms.Lambda(conv1d)])\n",
    "\n",
    "kwargs = {'num_workers': 20, 'pin_memory': True} if args.cuda else {}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./ML_data', train=True, download=True,\n",
    "                   transform=transforms_distort),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./ML_data', train=False, \n",
    "                   transform=transforms_distort),\n",
    "    batch_size=args.test_batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Definition of digitized fully connected layers \"\"\"\n",
    "\n",
    "def Digitize(tensor, quant_mode='det', levels=16, min_val=None, max_val=None):\n",
    "    if not min_val and not max_val:\n",
    "        min_val, max_val = tensor.min(), tensor.max()\n",
    "    tensor.clamp_(min_val, max_val).add_(-1*min_val).mul_(levels-1).div_(max_val-min_val)\n",
    "    if quant_mode == \"det\": \n",
    "        tensor.round_()\n",
    "    elif quant_mode == \"rand\":\n",
    "        tensor.add_(torch.rand(tensor.size(), device=tensor.device).add_(-0.5)).round_()\n",
    "    tensor.mul_(max_val-min_val).div_(levels-1).add_(min_val)\n",
    "    return tensor\n",
    "\n",
    "class DigitizeLinear(nn.Linear):\n",
    "\n",
    "    def __init__(self,  *kargs, a_quant_mode=\"det\", w_quant_mode=\"det\", a_quant_levels=16, w_quant_levels=32, running_weight=0.001, **kwargs):\n",
    "        super(DigitizeLinear, self).__init__(*kargs, **kwargs)\n",
    "        self.act_quant_mode = a_quant_mode\n",
    "        self.weight_quant_mode = w_quant_mode\n",
    "        self.register_buffer(\"act_quant_levels\", torch.tensor(a_quant_levels))\n",
    "        self.register_buffer(\"weight_quant_levels\", torch.tensor(w_quant_levels))\n",
    "        self.register_buffer(\"running_weight\", torch.tensor(running_weight)) \n",
    "        self.register_buffer(\"running_min\", None)\n",
    "        self.register_buffer(\"running_max\", None)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        if not self.weight_quant_mode is None: # Set a flag to control weight digitization.\n",
    "            if not hasattr(self.weight,'org'):\n",
    "                self.weight.org=self.weight.data.clone()\n",
    "            self.weight.data=Digitize(self.weight.data, quant_mode=self.weight_quant_mode, levels=self.weight_quant_levels)\n",
    "\n",
    "        if not self.bias is None:\n",
    "            self.bias.org=self.bias.data.clone()    \n",
    "        out = nn.functional.linear(input, self.weight, bias=self.bias)\n",
    "\n",
    "        if not self.act_quant_mode is None: # A flag to control output digitization. \n",
    "            if self.training: # Update the running average of min and max only during training\n",
    "                with torch.no_grad():\n",
    "                    if not self.running_min and not self.running_max:\n",
    "                        self.running_min, self.running_max = out.min(), out.max()\n",
    "                    self.running_min = (1-self.running_weight) * self.running_min + self.running_weight * out.min()\n",
    "                    self.running_max = (1-self.running_weight) * self.running_max + self.running_weight * out.max()\n",
    "            out.data=Digitize(out.data, quant_mode=self.act_quant_mode, levels=self.act_quant_levels, min_val=self.running_min, max_val=self.running_max)\n",
    "    \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Definition of QAT NN structure \"\"\"\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, Nunits, **kwargs):\n",
    "        super().__init__()\n",
    "        self.fcs = nn.ModuleList([DigitizeLinear(i,j,**kwargs) for i, j in zip(Nunits[:-1], Nunits[1:])])\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X.view(X.size(0), -1)\n",
    "        for i, fc in enumerate(self.fcs):\n",
    "            X = fc(X)\n",
    "            if fc is not self.fcs[-1]:\n",
    "                X = F.relu(X)\n",
    "        return X\n",
    "    \n",
    "    def set_digitize_config(self, a_quant_mode, w_quant_mode, a_quant_levels, w_quant_levels):\n",
    "        for fc in self.fcs:\n",
    "            fc.act_quant_mode = a_quant_mode\n",
    "            fc.weight_quant_mode = w_quant_mode\n",
    "            fc.act_quant_levels = torch.tensor(a_quant_levels)\n",
    "            fc.weight_quant_levels = torch.tensor(w_quant_levels)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " \"\"\" helper functions and classes \"\"\"\n",
    "    \n",
    "# A manager for dynamical book-keeping of the top k accuracies and model checkpoints during training\n",
    "class top_k_manager(object):\n",
    "    def __init__(self, k=10):\n",
    "        self.k_best = k\n",
    "        self.top_k_metric =[0.0]*self.k_best\n",
    "        self.top_k_paths = [\"\"]*self.k_best\n",
    "    \n",
    "    # Compare the new_metric to the top k metrics in the past, and find its place.\n",
    "    def update_rank(self, new_metric, path_keeping):\n",
    "        for rank, record_metric in enumerate(self.top_k_metric):\n",
    "            if record_metric <= new_metric:    \n",
    "                if os.path.exists(self.top_k_paths[-1]):\n",
    "                    os.remove(self.top_k_paths[-1])\n",
    "                if rank < self.k_best - 1:\n",
    "                    self.top_k_metric[rank+1:] = self.top_k_metric[rank:-1]\n",
    "                    self.top_k_paths[rank+1:] = self.top_k_paths[rank:-1]  \n",
    "                self.top_k_metric[rank] = new_metric\n",
    "                self.top_k_paths[rank] = path_keeping\n",
    "                return True # the top k list has been updated\n",
    "        return False\n",
    "    \n",
    "# A simple hook class that returns the input and output of a layer during forward/backward pass\n",
    "class Hook():\n",
    "    def __init__(self, module, backward=False):\n",
    "        if backward==False:\n",
    "            self.hook = module.register_forward_hook(self.hook_fn)\n",
    "        else:\n",
    "            self.hook = module.register_backward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.input = input\n",
    "        self.output = output\n",
    "    def close(self):\n",
    "        self.hook.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of Training and Testing Loops\n",
    "\n",
    "    Explanation on quantization-aware training algorithm used in train():\n",
    "    1. The activations are calculated with a forward passing, involving only quantized weights and activations. Meanwhile, the non-quantized version of the weights is still kept in memory for later use..\n",
    "    2. The gradients are calculated with backprop based on the quantized activations and weights calucated in (1). \n",
    "    3. The non-quantized version of parameters (weights + biases) are updated with gradients, and saved without quantization. Quantizating parameters immediately after updating them can erase small updates.\n",
    "    4. Quantization is only later performed on these parameters during the evaluation of activations in forward passing or errors in backprop. During these evaluation steps, a quantized copy of the non-quantized parameters are used. Meanwhile, the original non-quantized version stays unchanged until updated with the next batch of calculated gradients.\n",
    "    PS: the clipping of the parameters represents the straight-through esimator across hard tanh nonlinear layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer, criterion):\n",
    "    model.train()\n",
    "    # Loop around mini-batches in an epoch\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if args.cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        \"\"\" For an explanation of the parameter update below, see explanation above. \"\"\"\n",
    "        for p in list(model.parameters()):\n",
    "            if hasattr(p,'org'):\n",
    "                p.data.copy_(p.org)\n",
    "        optimizer.step()\n",
    "        for p in list(model.parameters()):\n",
    "            if hasattr(p,'org'):\n",
    "                p.org.copy_(p.data.clamp_(-1,1))\n",
    "        \"\"\"\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}\"\n",
    "            +f\" ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "        \"\"\"\n",
    "        if args.wandb:\n",
    "            wandb.log({\"train_loss\": loss.item(), \"batch\": batch_idx}, step = epoch)\n",
    "\n",
    "def test(epoch, model, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    fc_hks = [Hook(layer) for layer in model.fcs]\n",
    "    # Loop around mini-batches in an epoch\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            if args.cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() # sum up batch loss\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    val_stats = {\"val_loss\": test_loss, \"accuracy\": accuracy}\n",
    "    for i, hk in enumerate(fc_hks):\n",
    "        val_stats[f\"fc{i+1}\"] = wandb.Histogram(fc_hks[i].output.cpu())      \n",
    "            \n",
    "    print(f\"\\nTest set: Epoch {epoch}, Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)}\" \n",
    "          +f\"({accuracy:.0f}%)\\n\")\n",
    "\n",
    "    if args.wandb:\n",
    "        wandb.log(val_stats, step=epoch)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The objective function runs a trial in a NAS study (a loop around epochs) \"\"\"\n",
    "\n",
    "def objective(trial, NAS_project_name):\n",
    "\n",
    "    # Define the hyperparameter search space\n",
    "    fc1_units = trial.suggest_categorical(\"fc1_units\", [100, ])\n",
    "    fc2_units = 200 - fc1_units # aka, the 1st and 2nd hidden layers both have 100 units.\n",
    "    Nunits = [28**2, fc1_units, fc2_units, 10]\n",
    "    learning_rate = trial.suggest_uniform(\"lr\", 0.03, 0.05)\n",
    "    momentum = trial.suggest_uniform(\"mm\", 0.7, 1)\n",
    "    lr_decay = trial.suggest_uniform(\"lr_decay\", 0.3, 0.5)\n",
    "    warmup_epochs = trial.suggest_categorical(\"warmup_eps\", [6, 8, 10, 12])\n",
    "    model_description = f\"4bit_H2_{fc1_units}_{fc2_units}_lr_{learning_rate:.3f}_{lr_decay:.2f}\" + f\"_m_{momentum:.2f}\" + f\"_wep_{warmup_epochs}\" + \"_randActDigi\" + f\"_v_{trial.number}\"\n",
    "\n",
    "    # Instantiate a MLP model\n",
    "    model = Net(Nunits, a_quant_mode=None, w_quant_mode=None, a_quant_levels=16, w_quant_levels=32)\n",
    "    if args.cuda:\n",
    "        gpu_id = get_gpu_id()\n",
    "        print(f\"cuda:{gpu_id} available\")\n",
    "        torch.cuda.set_device(gpu_id)\n",
    "        model.cuda() # transfer the model from cpu to gpu\n",
    "    \n",
    "    # Set up logging if necessary\n",
    "    if args.wandb:\n",
    "        wandb.init(project=NAS_project_name, name=model_description, reinit=True)\n",
    "        wandb.watch(model, log=\"all\")\n",
    "\n",
    "    # Configure loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    #optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "    # Loop around epoches\n",
    "    tpk_mngr = top_k_manager()\n",
    "    ckpt_save_path = \"./\" + model_description\n",
    "    if not os.path.exists(ckpt_save_path):\n",
    "        os.makedirs(ckpt_save_path)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        if epoch > warmup_epochs:\n",
    "            model.set_digitize_config(\"rand\", \"det\", 16, 32)\n",
    "        train(epoch, model, optimizer, criterion)\n",
    "        loss, accu = test(epoch, model, criterion)\n",
    "        # schedule learning rate decay\n",
    "        if epoch%20==0:\n",
    "            optimizer.param_groups[0]['lr']=optimizer.param_groups[0]['lr'] * (lr_decay)\n",
    "        # Save the best models aftering the training gets more stable\n",
    "        if epoch > 20:\n",
    "            if tpk_mngr.update_rank(accu, ckpt_save_path+f\"/ep{epoch}.pt\"):\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss,\n",
    "                }, ckpt_save_path+f\"/ep{epoch}.pt\")\n",
    "\n",
    "    # Log the best models\n",
    "    trial.set_user_attr('top 3 accuracy', torch.tensor(tpk_mngr.top_k_metric[:3]).mean().item()) # Save the best accuracy during the taining loop      \n",
    "    if args.wandb:\n",
    "        wandb.run.summary[\"top_k_accu\"] = tpk_mngr.top_k_metric\n",
    "        wandb.run.summary[\"top_k_paths\"] = tpk_mngr.top_k_paths\n",
    "    return torch.tensor(tpk_mngr.top_k_metric[:5]).mean().item() # return the average of top k accuracies to guide NAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" The main function conducts an optuna neural architecture search (NAS) study (loop around hyperparameters) \"\"\"\n",
    "\n",
    "def hyper_run(rseed):\n",
    "    if args.wandb:\n",
    "        wandb.config = args\n",
    "    NAS_project_name = \"ParamSearch_mlp_4bit_QAT_H2_200_NAS\"\n",
    "    sampler = optuna.samplers.TPESampler(seed=rseed) \n",
    "    storage = f'sqlite:///'+NAS_project_name+'.db' # way to specify an SQL database\n",
    "    study = optuna.create_study(study_name=NAS_project_name, storage=storage, \n",
    "                                sampler=sampler, direction=\"maximize\", load_if_exists=True) \n",
    "    study.optimize(lambda trial: objective(trial, NAS_project_name), n_trials=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-17 12:35:03,866\tWARNING worker.py:1072 -- Warning: The remote function __main__.f_gpu has size 55452954 when pickled. It will be stored in Redis, which could cause memory issues. This may mean that its definition uses a large array or other object.\n",
      "(pid=32047) [I 2020-11-17 12:35:12,447] A new study created with name: ParamSearch_mlp_randy_aug_4bit_QAT_H2_180_NAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(pid=32047) cuda:3 available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(pid=32071) [I 2020-11-17 12:35:14,012] Using an existing study with name 'ParamSearch_mlp_randy_aug_4bit_QAT_H2_180_NAS' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(pid=32071) cuda:3 available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(pid=31949) [I 2020-11-17 12:35:15,123] Using an existing study with name 'ParamSearch_mlp_randy_aug_4bit_QAT_H2_180_NAS' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(pid=31949) cuda:3 available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(pid=32094) [I 2020-11-17 12:35:15,652] Using an existing study with name 'ParamSearch_mlp_randy_aug_4bit_QAT_H2_180_NAS' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(pid=32094) cuda:3 available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(pid=32047) wandb: Tracking run with wandb version 0.9.7\n",
      "(pid=32071) wandb: Tracking run with wandb version 0.9.7\n",
      "(pid=32047) wandb: Wandb version 0.10.10 is available!  To upgrade, please run:\n",
      "(pid=32047) wandb:  $ pip install wandb --upgrade\n",
      "(pid=32047) wandb: Run data is saved locally in wandb/run-20201117_123518-378fv7qm\n",
      "(pid=32047) wandb: Syncing run 4bit_H2_168_12_lr_0.031_0.32_m_0.87_wep_8_randActDigi_v_0\n",
      "(pid=32047) wandb: ⭐️ View project at https://app.wandb.ai/gangsterkitty/ParamSearch_mlp_randy_aug_4bit_QAT_H2_180_NAS\n",
      "(pid=32047) wandb: 🚀 View run at https://app.wandb.ai/gangsterkitty/ParamSearch_mlp_randy_aug_4bit_QAT_H2_180_NAS/runs/378fv7qm\n",
      "(pid=32047) wandb: Run `wandb off` to turn off syncing.\n",
      "(pid=32047) \n",
      "(pid=32071) wandb: Wandb version 0.10.10 is available!  To upgrade, please run:\n",
      "(pid=32071) wandb:  $ pip install wandb --upgrade\n",
      "(pid=32071) wandb: Run data is saved locally in wandb/run-20201117_123519-f2626jk6\n",
      "(pid=32094) wandb: Tracking run with wandb version 0.9.7\n",
      "(pid=31949) wandb: Tracking run with wandb version 0.9.7\n",
      "(pid=32071) wandb: Syncing run 4bit_H2_156_24_lr_0.050_0.33_m_0.89_wep_8_randActDigi_v_1\n",
      "(pid=32071) wandb: ⭐️ View project at https://app.wandb.ai/gangsterkitty/ParamSearch_mlp_randy_aug_4bit_QAT_H2_180_NAS\n",
      "(pid=32071) wandb: 🚀 View run at https://app.wandb.ai/gangsterkitty/ParamSearch_mlp_randy_aug_4bit_QAT_H2_180_NAS/runs/f2626jk6\n",
      "(pid=32071) wandb: Run `wandb off` to turn off syncing.\n",
      "(pid=32071) \n",
      "(pid=31949) wandb: Wandb version 0.10.10 is available!  To upgrade, please run:\n",
      "(pid=31949) wandb:  $ pip install wandb --upgrade\n",
      "(pid=31949) wandb: Run data is saved locally in wandb/run-20201117_123519-8za1678a\n",
      "(pid=31949) wandb: Syncing run 4bit_H2_156_24_lr_0.047_0.47_m_0.87_wep_12_randActDigi_v_2\n",
      "(pid=31949) wandb: ⭐️ View project at https://app.wandb.ai/gangsterkitty/ParamSearch_mlp_randy_aug_4bit_QAT_H2_180_NAS\n",
      "(pid=31949) wandb: 🚀 View run at https://app.wandb.ai/gangsterkitty/ParamSearch_mlp_randy_aug_4bit_QAT_H2_180_NAS/runs/8za1678a\n",
      "(pid=31949) wandb: Run `wandb off` to turn off syncing.\n",
      "(pid=31949) \n",
      "(pid=32094) wandb: Wandb version 0.10.10 is available!  To upgrade, please run:\n",
      "(pid=32094) wandb:  $ pip install wandb --upgrade\n",
      "(pid=32094) wandb: Run data is saved locally in wandb/run-20201117_123519-37cimam6\n",
      "(pid=32094) wandb: Syncing run 4bit_H2_164_16_lr_0.034_0.49_m_0.89_wep_12_randActDigi_v_3\n",
      "(pid=32094) wandb: ⭐️ View project at https://app.wandb.ai/gangsterkitty/ParamSearch_mlp_randy_aug_4bit_QAT_H2_180_NAS\n",
      "(pid=32094) wandb: 🚀 View run at https://app.wandb.ai/gangsterkitty/ParamSearch_mlp_randy_aug_4bit_QAT_H2_180_NAS/runs/37cimam6\n",
      "(pid=32094) wandb: Run `wandb off` to turn off syncing.\n",
      "(pid=32094) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 1, Average loss: 0.0002, Accuracy: 9436/10000(94%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 1, Average loss: 0.0002, Accuracy: 9477/10000(95%)\n",
      "(pid=32071) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 1, Average loss: 0.0002, Accuracy: 9310/10000(93%)\n",
      "(pid=31949) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 1, Average loss: 0.0002, Accuracy: 9387/10000(94%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 2, Average loss: 0.0001, Accuracy: 9550/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 2, Average loss: 0.0001, Accuracy: 9583/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 2, Average loss: 0.0001, Accuracy: 9668/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 2, Average loss: 0.0001, Accuracy: 9555/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 3, Average loss: 0.0001, Accuracy: 9642/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 3, Average loss: 0.0001, Accuracy: 9634/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 3, Average loss: 0.0001, Accuracy: 9653/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 3, Average loss: 0.0001, Accuracy: 9659/10000(97%)\n",
      "(pid=31949) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 4, Average loss: 0.0001, Accuracy: 9667/10000(97%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 4, Average loss: 0.0001, Accuracy: 9700/10000(97%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 4, Average loss: 0.0001, Accuracy: 9650/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 4, Average loss: 0.0001, Accuracy: 9639/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 5, Average loss: 0.0001, Accuracy: 9718/10000(97%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 5, Average loss: 0.0001, Accuracy: 9627/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 5, Average loss: 0.0001, Accuracy: 9711/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 5, Average loss: 0.0001, Accuracy: 9719/10000(97%)\n",
      "(pid=31949) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 6, Average loss: 0.0001, Accuracy: 9733/10000(97%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 6, Average loss: 0.0001, Accuracy: 9737/10000(97%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 6, Average loss: 0.0001, Accuracy: 9738/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 6, Average loss: 0.0001, Accuracy: 9680/10000(97%)\n",
      "(pid=31949) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 7, Average loss: 0.0001, Accuracy: 9735/10000(97%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 7, Average loss: 0.0001, Accuracy: 9736/10000(97%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 7, Average loss: 0.0001, Accuracy: 9684/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 8, Average loss: 0.0001, Accuracy: 9743/10000(97%)\n",
      "(pid=32047) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 7, Average loss: 0.0001, Accuracy: 9716/10000(97%)\n",
      "(pid=31949) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 8, Average loss: 0.0001, Accuracy: 9695/10000(97%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 8, Average loss: 0.0001, Accuracy: 9731/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 8, Average loss: 0.0001, Accuracy: 9744/10000(97%)\n",
      "(pid=31949) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 9, Average loss: 0.0001, Accuracy: 9731/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 9, Average loss: 0.0002, Accuracy: 9479/10000(95%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 9, Average loss: 0.0002, Accuracy: 9487/10000(95%)\n",
      "(pid=32071) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 9, Average loss: 0.0001, Accuracy: 9759/10000(98%)\n",
      "(pid=31949) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 10, Average loss: 0.0001, Accuracy: 9767/10000(98%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 10, Average loss: 0.0002, Accuracy: 9432/10000(94%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 10, Average loss: 0.0002, Accuracy: 9441/10000(94%)\n",
      "(pid=32071) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 10, Average loss: 0.0001, Accuracy: 9738/10000(97%)\n",
      "(pid=31949) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 11, Average loss: 0.0001, Accuracy: 9769/10000(98%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 11, Average loss: 0.0002, Accuracy: 9510/10000(95%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 11, Average loss: 0.0002, Accuracy: 9420/10000(94%)\n",
      "(pid=32071) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 11, Average loss: 0.0001, Accuracy: 9767/10000(98%)\n",
      "(pid=31949) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 12, Average loss: 0.0001, Accuracy: 9768/10000(98%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 12, Average loss: 0.0001, Accuracy: 9736/10000(97%)\n",
      "(pid=31949) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 12, Average loss: 0.0002, Accuracy: 9450/10000(94%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 12, Average loss: 0.0003, Accuracy: 9444/10000(94%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 13, Average loss: 0.0002, Accuracy: 9504/10000(95%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 13, Average loss: 0.0002, Accuracy: 9432/10000(94%)\n",
      "(pid=32047) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 13, Average loss: 0.0002, Accuracy: 9355/10000(94%)\n",
      "(pid=31949) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 13, Average loss: 0.0003, Accuracy: 9446/10000(94%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 14, Average loss: 0.0002, Accuracy: 9513/10000(95%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 14, Average loss: 0.0002, Accuracy: 9560/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 14, Average loss: 0.0003, Accuracy: 9398/10000(94%)\n",
      "(pid=32071) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 14, Average loss: 0.0002, Accuracy: 9466/10000(95%)\n",
      "(pid=31949) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 15, Average loss: 0.0002, Accuracy: 9528/10000(95%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 15, Average loss: 0.0002, Accuracy: 9474/10000(95%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 15, Average loss: 0.0003, Accuracy: 9370/10000(94%)\n",
      "(pid=32071) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 15, Average loss: 0.0002, Accuracy: 9505/10000(95%)\n",
      "(pid=31949) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 16, Average loss: 0.0002, Accuracy: 9534/10000(95%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 16, Average loss: 0.0002, Accuracy: 9555/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 16, Average loss: 0.0004, Accuracy: 9312/10000(93%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 17, Average loss: 0.0002, Accuracy: 9580/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 16, Average loss: 0.0003, Accuracy: 9468/10000(95%)\n",
      "(pid=31949) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 17, Average loss: 0.0002, Accuracy: 9563/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 17, Average loss: 0.0003, Accuracy: 9462/10000(95%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 18, Average loss: 0.0002, Accuracy: 9518/10000(95%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 17, Average loss: 0.0002, Accuracy: 9488/10000(95%)\n",
      "(pid=31949) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 18, Average loss: 0.0002, Accuracy: 9469/10000(95%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 18, Average loss: 0.0004, Accuracy: 9341/10000(93%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 19, Average loss: 0.0002, Accuracy: 9505/10000(95%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 18, Average loss: 0.0003, Accuracy: 9486/10000(95%)\n",
      "(pid=31949) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 19, Average loss: 0.0002, Accuracy: 9565/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 19, Average loss: 0.0005, Accuracy: 9432/10000(94%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 20, Average loss: 0.0002, Accuracy: 9478/10000(95%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 19, Average loss: 0.0003, Accuracy: 9557/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 20, Average loss: 0.0002, Accuracy: 9605/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 20, Average loss: 0.0006, Accuracy: 9357/10000(94%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 21, Average loss: 0.0002, Accuracy: 9568/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 20, Average loss: 0.0003, Accuracy: 9439/10000(94%)\n",
      "(pid=31949) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 21, Average loss: 0.0002, Accuracy: 9646/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 21, Average loss: 0.0004, Accuracy: 9570/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 22, Average loss: 0.0002, Accuracy: 9587/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 21, Average loss: 0.0002, Accuracy: 9615/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 22, Average loss: 0.0002, Accuracy: 9558/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 22, Average loss: 0.0004, Accuracy: 9544/10000(95%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 23, Average loss: 0.0002, Accuracy: 9633/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 23, Average loss: 0.0002, Accuracy: 9640/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 22, Average loss: 0.0002, Accuracy: 9551/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 23, Average loss: 0.0005, Accuracy: 9481/10000(95%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 24, Average loss: 0.0002, Accuracy: 9559/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 24, Average loss: 0.0001, Accuracy: 9643/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 23, Average loss: 0.0003, Accuracy: 9590/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 24, Average loss: 0.0004, Accuracy: 9479/10000(95%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 25, Average loss: 0.0002, Accuracy: 9575/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 25, Average loss: 0.0002, Accuracy: 9627/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 25, Average loss: 0.0005, Accuracy: 9552/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 24, Average loss: 0.0003, Accuracy: 9565/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 26, Average loss: 0.0002, Accuracy: 9554/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 26, Average loss: 0.0002, Accuracy: 9597/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 26, Average loss: 0.0004, Accuracy: 9554/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 25, Average loss: 0.0002, Accuracy: 9631/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 27, Average loss: 0.0002, Accuracy: 9610/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 27, Average loss: 0.0002, Accuracy: 9614/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 27, Average loss: 0.0005, Accuracy: 9608/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 26, Average loss: 0.0003, Accuracy: 9584/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 28, Average loss: 0.0002, Accuracy: 9631/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 28, Average loss: 0.0002, Accuracy: 9636/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 28, Average loss: 0.0005, Accuracy: 9606/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 29, Average loss: 0.0002, Accuracy: 9567/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 27, Average loss: 0.0003, Accuracy: 9609/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 29, Average loss: 0.0002, Accuracy: 9621/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 29, Average loss: 0.0005, Accuracy: 9528/10000(95%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 30, Average loss: 0.0002, Accuracy: 9623/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 28, Average loss: 0.0003, Accuracy: 9574/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 30, Average loss: 0.0002, Accuracy: 9622/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 30, Average loss: 0.0005, Accuracy: 9567/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 31, Average loss: 0.0002, Accuracy: 9609/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 29, Average loss: 0.0003, Accuracy: 9649/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 31, Average loss: 0.0001, Accuracy: 9637/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 31, Average loss: 0.0006, Accuracy: 9607/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 32, Average loss: 0.0002, Accuracy: 9510/10000(95%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 30, Average loss: 0.0003, Accuracy: 9588/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 32, Average loss: 0.0002, Accuracy: 9664/10000(97%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 32, Average loss: 0.0007, Accuracy: 9575/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 33, Average loss: 0.0002, Accuracy: 9562/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 31, Average loss: 0.0003, Accuracy: 9599/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 33, Average loss: 0.0002, Accuracy: 9613/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 33, Average loss: 0.0006, Accuracy: 9563/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 34, Average loss: 0.0002, Accuracy: 9633/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 34, Average loss: 0.0002, Accuracy: 9671/10000(97%)\n",
      "(pid=32047) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 32, Average loss: 0.0003, Accuracy: 9559/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 34, Average loss: 0.0006, Accuracy: 9588/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 35, Average loss: 0.0002, Accuracy: 9615/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 35, Average loss: 0.0002, Accuracy: 9584/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 33, Average loss: 0.0003, Accuracy: 9616/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 35, Average loss: 0.0006, Accuracy: 9543/10000(95%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 36, Average loss: 0.0002, Accuracy: 9501/10000(95%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 36, Average loss: 0.0002, Accuracy: 9643/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 36, Average loss: 0.0007, Accuracy: 9544/10000(95%)\n",
      "(pid=32071) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 34, Average loss: 0.0003, Accuracy: 9457/10000(95%)\n",
      "(pid=31949) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 37, Average loss: 0.0002, Accuracy: 9528/10000(95%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 37, Average loss: 0.0002, Accuracy: 9635/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 37, Average loss: 0.0006, Accuracy: 9561/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 35, Average loss: 0.0003, Accuracy: 9560/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 38, Average loss: 0.0002, Accuracy: 9638/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 38, Average loss: 0.0002, Accuracy: 9628/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 38, Average loss: 0.0007, Accuracy: 9570/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 36, Average loss: 0.0003, Accuracy: 9628/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 39, Average loss: 0.0002, Accuracy: 9657/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 39, Average loss: 0.0002, Accuracy: 9615/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 37, Average loss: 0.0003, Accuracy: 9495/10000(95%)\n",
      "(pid=31949) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 40, Average loss: 0.0002, Accuracy: 9654/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=32071) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(pid=32071) Test set: Epoch 39, Average loss: 0.0007, Accuracy: 9496/10000(95%)\n",
      "(pid=32071) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 40, Average loss: 0.0002, Accuracy: 9608/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 41, Average loss: 0.0002, Accuracy: 9666/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 38, Average loss: 0.0003, Accuracy: 9567/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 40, Average loss: 0.0007, Accuracy: 9588/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 41, Average loss: 0.0001, Accuracy: 9682/10000(97%)\n",
      "(pid=32047) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 42, Average loss: 0.0002, Accuracy: 9595/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 42, Average loss: 0.0002, Accuracy: 9634/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 39, Average loss: 0.0003, Accuracy: 9624/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 41, Average loss: 0.0007, Accuracy: 9611/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 43, Average loss: 0.0002, Accuracy: 9670/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 43, Average loss: 0.0002, Accuracy: 9653/10000(97%)\n",
      "(pid=32047) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 40, Average loss: 0.0003, Accuracy: 9622/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 42, Average loss: 0.0007, Accuracy: 9577/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 44, Average loss: 0.0002, Accuracy: 9653/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 44, Average loss: 0.0002, Accuracy: 9618/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 41, Average loss: 0.0003, Accuracy: 9605/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 43, Average loss: 0.0006, Accuracy: 9606/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 45, Average loss: 0.0002, Accuracy: 9644/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 45, Average loss: 0.0002, Accuracy: 9621/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 42, Average loss: 0.0002, Accuracy: 9664/10000(97%)\n",
      "(pid=31949) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 46, Average loss: 0.0002, Accuracy: 9674/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 44, Average loss: 0.0006, Accuracy: 9609/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 46, Average loss: 0.0002, Accuracy: 9618/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 43, Average loss: 0.0002, Accuracy: 9647/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 47, Average loss: 0.0002, Accuracy: 9622/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 47, Average loss: 0.0001, Accuracy: 9633/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 45, Average loss: 0.0006, Accuracy: 9620/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 48, Average loss: 0.0002, Accuracy: 9636/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 48, Average loss: 0.0002, Accuracy: 9649/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 44, Average loss: 0.0002, Accuracy: 9599/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 46, Average loss: 0.0006, Accuracy: 9588/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 49, Average loss: 0.0002, Accuracy: 9617/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 49, Average loss: 0.0002, Accuracy: 9660/10000(97%)\n",
      "(pid=32047) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 45, Average loss: 0.0002, Accuracy: 9640/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 47, Average loss: 0.0006, Accuracy: 9650/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 50, Average loss: 0.0001, Accuracy: 9616/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 50, Average loss: 0.0002, Accuracy: 9631/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 46, Average loss: 0.0002, Accuracy: 9629/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 48, Average loss: 0.0006, Accuracy: 9566/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 51, Average loss: 0.0002, Accuracy: 9621/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 51, Average loss: 0.0002, Accuracy: 9649/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 47, Average loss: 0.0002, Accuracy: 9638/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 49, Average loss: 0.0006, Accuracy: 9571/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 52, Average loss: 0.0002, Accuracy: 9682/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 52, Average loss: 0.0001, Accuracy: 9658/10000(97%)\n",
      "(pid=32047) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 48, Average loss: 0.0002, Accuracy: 9672/10000(97%)\n",
      "(pid=31949) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 53, Average loss: 0.0002, Accuracy: 9611/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 53, Average loss: 0.0001, Accuracy: 9634/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 50, Average loss: 0.0006, Accuracy: 9571/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 49, Average loss: 0.0002, Accuracy: 9631/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 54, Average loss: 0.0002, Accuracy: 9599/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 54, Average loss: 0.0001, Accuracy: 9630/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 51, Average loss: 0.0006, Accuracy: 9594/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 55, Average loss: 0.0002, Accuracy: 9573/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 55, Average loss: 0.0001, Accuracy: 9607/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 50, Average loss: 0.0002, Accuracy: 9623/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 52, Average loss: 0.0006, Accuracy: 9593/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 56, Average loss: 0.0001, Accuracy: 9661/10000(97%)\n",
      "(pid=32047) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 56, Average loss: 0.0002, Accuracy: 9626/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 51, Average loss: 0.0002, Accuracy: 9662/10000(97%)\n",
      "(pid=31949) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 53, Average loss: 0.0007, Accuracy: 9551/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 57, Average loss: 0.0002, Accuracy: 9611/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 57, Average loss: 0.0002, Accuracy: 9659/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 52, Average loss: 0.0002, Accuracy: 9614/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 54, Average loss: 0.0006, Accuracy: 9577/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 58, Average loss: 0.0002, Accuracy: 9674/10000(97%)\n",
      "(pid=32047) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 58, Average loss: 0.0002, Accuracy: 9627/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 53, Average loss: 0.0002, Accuracy: 9583/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 55, Average loss: 0.0006, Accuracy: 9623/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 59, Average loss: 0.0001, Accuracy: 9631/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 59, Average loss: 0.0002, Accuracy: 9650/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 54, Average loss: 0.0002, Accuracy: 9631/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 60, Average loss: 0.0002, Accuracy: 9589/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 60, Average loss: 0.0002, Accuracy: 9656/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 56, Average loss: 0.0006, Accuracy: 9607/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 61, Average loss: 0.0001, Accuracy: 9660/10000(97%)\n",
      "(pid=32047) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 55, Average loss: 0.0002, Accuracy: 9667/10000(97%)\n",
      "(pid=31949) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 61, Average loss: 0.0002, Accuracy: 9645/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 57, Average loss: 0.0006, Accuracy: 9595/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 62, Average loss: 0.0002, Accuracy: 9638/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 62, Average loss: 0.0002, Accuracy: 9635/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 56, Average loss: 0.0002, Accuracy: 9566/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 58, Average loss: 0.0007, Accuracy: 9576/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 63, Average loss: 0.0001, Accuracy: 9636/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 63, Average loss: 0.0002, Accuracy: 9642/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 57, Average loss: 0.0002, Accuracy: 9588/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 59, Average loss: 0.0006, Accuracy: 9627/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 64, Average loss: 0.0002, Accuracy: 9620/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 64, Average loss: 0.0001, Accuracy: 9658/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 58, Average loss: 0.0002, Accuracy: 9622/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 65, Average loss: 0.0002, Accuracy: 9617/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 60, Average loss: 0.0006, Accuracy: 9594/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 65, Average loss: 0.0001, Accuracy: 9681/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 59, Average loss: 0.0002, Accuracy: 9692/10000(97%)\n",
      "(pid=31949) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 66, Average loss: 0.0001, Accuracy: 9629/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 61, Average loss: 0.0006, Accuracy: 9633/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 66, Average loss: 0.0001, Accuracy: 9676/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 60, Average loss: 0.0003, Accuracy: 9623/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 67, Average loss: 0.0002, Accuracy: 9635/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 67, Average loss: 0.0001, Accuracy: 9640/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 62, Average loss: 0.0006, Accuracy: 9594/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 68, Average loss: 0.0001, Accuracy: 9650/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 61, Average loss: 0.0002, Accuracy: 9626/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 68, Average loss: 0.0002, Accuracy: 9611/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 63, Average loss: 0.0006, Accuracy: 9628/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 69, Average loss: 0.0001, Accuracy: 9639/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 62, Average loss: 0.0002, Accuracy: 9597/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 69, Average loss: 0.0001, Accuracy: 9671/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 64, Average loss: 0.0005, Accuracy: 9615/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 70, Average loss: 0.0002, Accuracy: 9641/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 70, Average loss: 0.0001, Accuracy: 9670/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 63, Average loss: 0.0002, Accuracy: 9621/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 71, Average loss: 0.0002, Accuracy: 9633/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 65, Average loss: 0.0007, Accuracy: 9583/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 71, Average loss: 0.0001, Accuracy: 9635/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 64, Average loss: 0.0002, Accuracy: 9682/10000(97%)\n",
      "(pid=31949) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 72, Average loss: 0.0001, Accuracy: 9653/10000(97%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 66, Average loss: 0.0006, Accuracy: 9647/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 72, Average loss: 0.0001, Accuracy: 9632/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 65, Average loss: 0.0002, Accuracy: 9631/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 73, Average loss: 0.0001, Accuracy: 9644/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 67, Average loss: 0.0006, Accuracy: 9601/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 73, Average loss: 0.0001, Accuracy: 9678/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 74, Average loss: 0.0002, Accuracy: 9624/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 66, Average loss: 0.0002, Accuracy: 9652/10000(97%)\n",
      "(pid=31949) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 74, Average loss: 0.0001, Accuracy: 9660/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 68, Average loss: 0.0006, Accuracy: 9626/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 75, Average loss: 0.0001, Accuracy: 9637/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 67, Average loss: 0.0002, Accuracy: 9647/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 75, Average loss: 0.0001, Accuracy: 9674/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 69, Average loss: 0.0006, Accuracy: 9603/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 76, Average loss: 0.0002, Accuracy: 9624/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 76, Average loss: 0.0001, Accuracy: 9656/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 68, Average loss: 0.0002, Accuracy: 9655/10000(97%)\n",
      "(pid=31949) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 77, Average loss: 0.0002, Accuracy: 9610/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 70, Average loss: 0.0006, Accuracy: 9602/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 77, Average loss: 0.0001, Accuracy: 9685/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 69, Average loss: 0.0002, Accuracy: 9625/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 78, Average loss: 0.0001, Accuracy: 9647/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 71, Average loss: 0.0005, Accuracy: 9629/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 78, Average loss: 0.0001, Accuracy: 9651/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 79, Average loss: 0.0001, Accuracy: 9624/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 70, Average loss: 0.0002, Accuracy: 9600/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 79, Average loss: 0.0001, Accuracy: 9648/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 72, Average loss: 0.0006, Accuracy: 9601/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 80, Average loss: 0.0001, Accuracy: 9626/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 71, Average loss: 0.0002, Accuracy: 9663/10000(97%)\n",
      "(pid=31949) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 80, Average loss: 0.0001, Accuracy: 9668/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 81, Average loss: 0.0001, Accuracy: 9643/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 73, Average loss: 0.0006, Accuracy: 9624/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 81, Average loss: 0.0001, Accuracy: 9644/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 72, Average loss: 0.0002, Accuracy: 9646/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 82, Average loss: 0.0001, Accuracy: 9636/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(pid=32071) Test set: Epoch 74, Average loss: 0.0005, Accuracy: 9643/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 82, Average loss: 0.0001, Accuracy: 9684/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 73, Average loss: 0.0002, Accuracy: 9603/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 83, Average loss: 0.0001, Accuracy: 9670/10000(97%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 75, Average loss: 0.0006, Accuracy: 9645/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 83, Average loss: 0.0001, Accuracy: 9649/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 84, Average loss: 0.0002, Accuracy: 9616/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 74, Average loss: 0.0002, Accuracy: 9625/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 84, Average loss: 0.0001, Accuracy: 9646/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 76, Average loss: 0.0006, Accuracy: 9646/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 85, Average loss: 0.0001, Accuracy: 9639/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 75, Average loss: 0.0002, Accuracy: 9629/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 85, Average loss: 0.0001, Accuracy: 9671/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 77, Average loss: 0.0006, Accuracy: 9616/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 86, Average loss: 0.0001, Accuracy: 9616/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 76, Average loss: 0.0002, Accuracy: 9625/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 86, Average loss: 0.0002, Accuracy: 9657/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 87, Average loss: 0.0001, Accuracy: 9652/10000(97%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 78, Average loss: 0.0006, Accuracy: 9591/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 87, Average loss: 0.0001, Accuracy: 9654/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 77, Average loss: 0.0002, Accuracy: 9674/10000(97%)\n",
      "(pid=31949) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 88, Average loss: 0.0001, Accuracy: 9644/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 79, Average loss: 0.0006, Accuracy: 9616/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 88, Average loss: 0.0001, Accuracy: 9658/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 89, Average loss: 0.0001, Accuracy: 9633/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 78, Average loss: 0.0002, Accuracy: 9638/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 89, Average loss: 0.0001, Accuracy: 9653/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 80, Average loss: 0.0006, Accuracy: 9629/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 90, Average loss: 0.0002, Accuracy: 9647/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 79, Average loss: 0.0002, Accuracy: 9628/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 90, Average loss: 0.0001, Accuracy: 9653/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 81, Average loss: 0.0005, Accuracy: 9617/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 91, Average loss: 0.0001, Accuracy: 9640/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 80, Average loss: 0.0002, Accuracy: 9655/10000(97%)\n",
      "(pid=31949) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 91, Average loss: 0.0001, Accuracy: 9664/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 92, Average loss: 0.0002, Accuracy: 9649/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 82, Average loss: 0.0006, Accuracy: 9601/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 81, Average loss: 0.0002, Accuracy: 9662/10000(97%)\n",
      "(pid=31949) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 92, Average loss: 0.0001, Accuracy: 9645/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 93, Average loss: 0.0002, Accuracy: 9624/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 83, Average loss: 0.0005, Accuracy: 9611/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 93, Average loss: 0.0001, Accuracy: 9642/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 82, Average loss: 0.0002, Accuracy: 9660/10000(97%)\n",
      "(pid=31949) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 94, Average loss: 0.0001, Accuracy: 9635/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 84, Average loss: 0.0006, Accuracy: 9607/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 94, Average loss: 0.0001, Accuracy: 9666/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 83, Average loss: 0.0002, Accuracy: 9645/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 95, Average loss: 0.0001, Accuracy: 9625/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 95, Average loss: 0.0001, Accuracy: 9679/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 85, Average loss: 0.0006, Accuracy: 9645/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 96, Average loss: 0.0001, Accuracy: 9645/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 84, Average loss: 0.0002, Accuracy: 9646/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 96, Average loss: 0.0001, Accuracy: 9686/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 97, Average loss: 0.0001, Accuracy: 9652/10000(97%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 86, Average loss: 0.0006, Accuracy: 9614/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 85, Average loss: 0.0002, Accuracy: 9648/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 97, Average loss: 0.0001, Accuracy: 9644/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 98, Average loss: 0.0001, Accuracy: 9647/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 87, Average loss: 0.0006, Accuracy: 9587/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 86, Average loss: 0.0002, Accuracy: 9620/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 98, Average loss: 0.0001, Accuracy: 9621/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 99, Average loss: 0.0001, Accuracy: 9631/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 88, Average loss: 0.0006, Accuracy: 9576/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 99, Average loss: 0.0001, Accuracy: 9631/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 87, Average loss: 0.0002, Accuracy: 9629/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 100, Average loss: 0.0002, Accuracy: 9598/10000(96%)\n",
      "(pid=32047) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(pid=32047) [I 2020-11-17 12:59:13,328] Finished trial#0 with value: 96.72200012207031 with parameters: {'fc1_units': 168, 'lr': 0.03141449760021982, 'mm': 0.867989808754481, 'lr_decay': 0.3242657162410432, 'warmup_eps': 8}. Best is trial#0 with value: 96.72200012207031.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(pid=32047) cuda:3 available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(pid=32047) \n",
      "(pid=32047) wandb: Waiting for W&B process to finish, PID 42161\n",
      "(pid=32047) wandb: Program ended successfully.\n",
      "(pid=32047) wandb: Run summary:\n",
      "(pid=32047) wandb:                     batch 937\n",
      "(pid=32047) wandb:                  _runtime 1443.8451828956604\n",
      "(pid=32047) wandb:                  val_loss 0.00016083306819200517\n",
      "(pid=32047) wandb:                train_loss 0.0008957763202488422\n",
      "(pid=32047) wandb:                _timestamp 1605635953.0751202\n",
      "(pid=32047) wandb:                     _step 100\n",
      "(pid=32047) wandb:                  accuracy 95.9800033569336\n",
      "(pid=32047) wandb: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "wandb:                                                                                \n",
      "(pid=32047) wandb: Synced 4bit_H2_168_12_lr_0.031_0.32_m_0.87_wep_8_randActDigi_v_0: https://app.wandb.ai/gangsterkitty/ParamSearch_mlp_randy_aug_4bit_QAT_H2_180_NAS/runs/378fv7qm\n",
      "(pid=32047) wandb: Tracking run with wandb version 0.9.7\n",
      "(pid=32047) wandb: Wandb version 0.10.10 is available!  To upgrade, please run:\n",
      "(pid=32047) wandb:  $ pip install wandb --upgrade\n",
      "(pid=32047) wandb: Run data is saved locally in wandb/run-20201117_125918-ldvnxun9\n",
      "(pid=32047) wandb: Syncing run 4bit_H2_140_40_lr_0.048_0.34_m_0.73_wep_12_randActDigi_v_4\n",
      "(pid=32047) wandb: ⭐️ View project at https://app.wandb.ai/gangsterkitty/ParamSearch_mlp_randy_aug_4bit_QAT_H2_180_NAS\n",
      "(pid=32047) wandb: 🚀 View run at https://app.wandb.ai/gangsterkitty/ParamSearch_mlp_randy_aug_4bit_QAT_H2_180_NAS/runs/ldvnxun9\n",
      "(pid=32047) wandb: Run `wandb off` to turn off syncing.\n",
      "(pid=32047) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 89, Average loss: 0.0006, Accuracy: 9627/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 100, Average loss: 0.0001, Accuracy: 9654/10000(97%)\n",
      "(pid=32094) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(pid=32094) [I 2020-11-17 12:59:25,275] Finished trial#3 with value: 96.83599853515625 with parameters: {'fc1_units': 164, 'lr': 0.033701641563464134, 'mm': 0.8863081731642224, 'lr_decay': 0.4895461223055756, 'warmup_eps': 12}. Best is trial#3 with value: 96.83599853515625.\n",
      "(pid=32094) \n",
      "(pid=32094) wandb: Waiting for W&B process to finish, PID 42401\n",
      "(pid=32094) wandb: Program ended successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(pid=32094) cuda:3 available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(pid=32094) wandb: Run summary:\n",
      "(pid=32094) wandb:                  accuracy 96.54000091552734\n",
      "(pid=32094) wandb:                train_loss 0.06844605505466461\n",
      "(pid=32094) wandb:                     _step 100\n",
      "(pid=32094) wandb:                  val_loss 0.00014039584398269653\n",
      "(pid=32094) wandb:                     batch 937\n",
      "(pid=32094) wandb:                _timestamp 1605635965.0620441\n",
      "(pid=32094) wandb:                  _runtime 1452.0894711017609\n",
      "(pid=32094) wandb: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "wandb:                                                                                \n",
      "(pid=32094) wandb: Synced 4bit_H2_164_16_lr_0.034_0.49_m_0.89_wep_12_randActDigi_v_3: https://app.wandb.ai/gangsterkitty/ParamSearch_mlp_randy_aug_4bit_QAT_H2_180_NAS/runs/37cimam6\n",
      "(pid=32094) wandb: Tracking run with wandb version 0.9.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 88, Average loss: 0.0002, Accuracy: 9651/10000(97%)\n",
      "(pid=31949) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(pid=32094) wandb: Wandb version 0.10.10 is available!  To upgrade, please run:\n",
      "(pid=32094) wandb:  $ pip install wandb --upgrade\n",
      "(pid=32094) wandb: Run data is saved locally in wandb/run-20201117_125929-2caehkfe\n",
      "(pid=32094) wandb: Syncing run 4bit_H2_164_16_lr_0.036_0.44_m_0.73_wep_12_randActDigi_v_5\n",
      "(pid=32094) wandb: ⭐️ View project at https://app.wandb.ai/gangsterkitty/ParamSearch_mlp_randy_aug_4bit_QAT_H2_180_NAS\n",
      "(pid=32094) wandb: 🚀 View run at https://app.wandb.ai/gangsterkitty/ParamSearch_mlp_randy_aug_4bit_QAT_H2_180_NAS/runs/2caehkfe\n",
      "(pid=32094) wandb: Run `wandb off` to turn off syncing.\n",
      "(pid=32094) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 1, Average loss: 0.0002, Accuracy: 9255/10000(93%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 90, Average loss: 0.0006, Accuracy: 9625/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 2, Average loss: 0.0001, Accuracy: 9566/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 1, Average loss: 0.0002, Accuracy: 9253/10000(93%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 89, Average loss: 0.0002, Accuracy: 9632/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 2, Average loss: 0.0002, Accuracy: 9524/10000(95%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 3, Average loss: 0.0001, Accuracy: 9645/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 91, Average loss: 0.0006, Accuracy: 9600/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 90, Average loss: 0.0002, Accuracy: 9638/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 3, Average loss: 0.0001, Accuracy: 9620/10000(96%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 4, Average loss: 0.0001, Accuracy: 9692/10000(97%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 92, Average loss: 0.0006, Accuracy: 9573/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 5, Average loss: 0.0001, Accuracy: 9733/10000(97%)\n",
      "(pid=32047) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 4, Average loss: 0.0001, Accuracy: 9681/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 91, Average loss: 0.0002, Accuracy: 9660/10000(97%)\n",
      "(pid=31949) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 6, Average loss: 0.0001, Accuracy: 9663/10000(97%)\n",
      "(pid=32047) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 5, Average loss: 0.0001, Accuracy: 9720/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 93, Average loss: 0.0005, Accuracy: 9616/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 92, Average loss: 0.0002, Accuracy: 9640/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 7, Average loss: 0.0001, Accuracy: 9746/10000(97%)\n",
      "(pid=32047) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 6, Average loss: 0.0001, Accuracy: 9697/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 8, Average loss: 0.0001, Accuracy: 9759/10000(98%)\n",
      "(pid=32047) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 94, Average loss: 0.0005, Accuracy: 9615/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 7, Average loss: 0.0001, Accuracy: 9692/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 93, Average loss: 0.0002, Accuracy: 9650/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 9, Average loss: 0.0001, Accuracy: 9735/10000(97%)\n",
      "(pid=32047) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 8, Average loss: 0.0001, Accuracy: 9727/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 95, Average loss: 0.0006, Accuracy: 9581/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 10, Average loss: 0.0001, Accuracy: 9774/10000(98%)\n",
      "(pid=32047) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 94, Average loss: 0.0002, Accuracy: 9641/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 9, Average loss: 0.0001, Accuracy: 9752/10000(98%)\n",
      "(pid=32094) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 11, Average loss: 0.0001, Accuracy: 9769/10000(98%)\n",
      "(pid=32047) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 10, Average loss: 0.0001, Accuracy: 9749/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 96, Average loss: 0.0006, Accuracy: 9587/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 95, Average loss: 0.0002, Accuracy: 9671/10000(97%)\n",
      "(pid=31949) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 12, Average loss: 0.0001, Accuracy: 9792/10000(98%)\n",
      "(pid=32047) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 11, Average loss: 0.0001, Accuracy: 9736/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 97, Average loss: 0.0006, Accuracy: 9595/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 13, Average loss: 0.0002, Accuracy: 9492/10000(95%)\n",
      "(pid=32047) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 12, Average loss: 0.0001, Accuracy: 9732/10000(97%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 96, Average loss: 0.0002, Accuracy: 9647/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 14, Average loss: 0.0001, Accuracy: 9550/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 13, Average loss: 0.0001, Accuracy: 9546/10000(95%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 97, Average loss: 0.0002, Accuracy: 9627/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 98, Average loss: 0.0005, Accuracy: 9587/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 15, Average loss: 0.0002, Accuracy: 9528/10000(95%)\n",
      "(pid=32047) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 14, Average loss: 0.0001, Accuracy: 9528/10000(95%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 98, Average loss: 0.0002, Accuracy: 9631/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 99, Average loss: 0.0006, Accuracy: 9593/10000(96%)\n",
      "(pid=32071) \n",
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 16, Average loss: 0.0002, Accuracy: 9409/10000(94%)\n",
      "(pid=32047) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 15, Average loss: 0.0002, Accuracy: 9473/10000(95%)\n",
      "(pid=32094) \n",
      "(pid=31949) \n",
      "(pid=31949) Test set: Epoch 99, Average loss: 0.0002, Accuracy: 9612/10000(96%)\n",
      "(pid=31949) \n",
      "(pid=32071) \n",
      "(pid=32071) Test set: Epoch 100, Average loss: 0.0006, Accuracy: 9606/10000(96%)\n",
      "(pid=32071) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(pid=32071) [I 2020-11-17 13:03:10,474] Finished trial#1 with value: 96.46600341796875 with parameters: {'fc1_units': 156, 'lr': 0.049943696218777374, 'mm': 0.8865114718677318, 'lr_decay': 0.3256248895858713, 'warmup_eps': 8}. Best is trial#3 with value: 96.83599853515625.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(pid=32071) cuda:3 available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(pid=32071) \n",
      "(pid=32071) wandb: Waiting for W&B process to finish, PID 42256\n",
      "(pid=32071) wandb: Program ended successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(pid=32047) \n",
      "(pid=32047) Test set: Epoch 17, Average loss: 0.0002, Accuracy: 9599/10000(96%)\n",
      "(pid=32047) \n",
      "(pid=32094) \n",
      "(pid=32094) Test set: Epoch 16, Average loss: 0.0002, Accuracy: 9525/10000(95%)\n",
      "(pid=32094) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(pid=32071) wandb: Run summary:\n",
      "(pid=32071) wandb:                train_loss 0.021660849452018738\n",
      "(pid=32071) wandb:                     _step 100\n",
      "(pid=32071) wandb:                _timestamp 1605636190.2000477\n",
      "(pid=32071) wandb:                  accuracy 96.05999755859375\n",
      "(pid=32071) wandb:                     batch 937\n",
      "(pid=32071) wandb:                  _runtime 1678.9753375053406\n",
      "(pid=32071) wandb:                  val_loss 0.0005887947976589203\n",
      "(pid=32071) wandb: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "wandb:                                                                                \n",
      "(pid=32071) wandb: Synced 4bit_H2_156_24_lr_0.050_0.33_m_0.89_wep_8_randActDigi_v_1: https://app.wandb.ai/gangsterkitty/ParamSearch_mlp_randy_aug_4bit_QAT_H2_180_NAS/runs/f2626jk6\n",
      "(pid=32071) wandb: Tracking run with wandb version 0.9.7\n",
      "(pid=32071) wandb: Wandb version 0.10.10 is available!  To upgrade, please run:\n",
      "(pid=32071) wandb:  $ pip install wandb --upgrade\n",
      "(pid=32071) wandb: Run data is saved locally in wandb/run-20201117_130316-1w0lyyek\n"
     ]
    }
   ],
   "source": [
    "# Start parallel parameter seaching in multiple GPUs\n",
    "gpu_map(hyper_run, range(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ObjectRef(df5a1a828c9685d3ffffffff0100000001000000),\n",
       " ObjectRef(cb230a572350ff44ffffffff0100000001000000),\n",
       " ObjectRef(7bbd90284b71e599ffffffff0100000001000000),\n",
       " ObjectRef(bd37d2621480fc7dffffffff0100000001000000)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the instances of training threads (each is an optuna project) in GPU\n",
    "ray_objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Terminate all current GPU threads. \n",
    "# Note: just stopping Jupyter notebook wont stop the threads.\n",
    "kill_gpu_processes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
